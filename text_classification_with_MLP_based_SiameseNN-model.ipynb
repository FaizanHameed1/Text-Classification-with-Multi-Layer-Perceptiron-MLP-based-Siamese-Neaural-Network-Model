{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/umar_visionx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout,Lambda, Flatten,Activation\n",
    "from tensorflow.keras.layers import BatchNormalization,Input,LeakyReLU\n",
    "from tensorflow.keras.backend import abs,sqrt,sum,maximum,square,epsilon\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer #tokanization and removel of punctuation \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special character and numbers\n",
    "def rem_sp_char(words):\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]'  # defined pattern to keep\n",
    "    words=re.sub(pattern, '', words)\n",
    "    return words\n",
    "\n",
    "#tokanizing and removing punctuations\n",
    "def remove_punct(words):\n",
    "  tokenizer = RegexpTokenizer(r'\\w+')\n",
    "  tokens=tokenizer.tokenize(words)\n",
    "  return tokens\n",
    "\n",
    "#lemmitization\n",
    "def lemmitizar(words):\n",
    "  lemmatized_words=[]\n",
    "  l = WordNetLemmatizer()\n",
    "  for word in words:\n",
    "      token = l.lemmatize(word)\n",
    "      lemmatized_words.append(token)\n",
    "  return lemmatized_words\n",
    "\n",
    "#removing stop-words\n",
    "def remove_stopwords(words):\n",
    "  filter_words = []\n",
    "  Stopwords = stopwords.words('english')\n",
    "  for word in words:\n",
    "      if word not in Stopwords:\n",
    "          filter_words.append(word)\n",
    "  return filter_words\n",
    "\n",
    "#converting to lowercase\n",
    "def lower_case(words):\n",
    "  lower_words=[]\n",
    "  for word in words:\n",
    "    lower_words.append(word.lower())\n",
    "  return lower_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='/media/umar_visionx/Backup Plus/Active/Faizan/dataset'\n",
    "#data_path='/media/umar_visionx/Backup Plus/Active/Faizan/test_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dictionaries and lists of text and labels\n",
    "data_dict={}\n",
    "#txt_file_path=[]\n",
    "labels=[]\n",
    "text=[]\n",
    "for clss in os.listdir(data_path):\n",
    " clss_path=os.path.join(data_path,clss)\n",
    " for txt_file in os.listdir(clss_path):\n",
    "    file_path=os.path.join(clss_path,txt_file)\n",
    "    #reading text file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        para = file.read().rstrip('\\n')#rstrip method removes trailing character based on string argument passed\n",
    "\n",
    "        #apply preprocessing functions on para\n",
    "        words = rem_sp_char(para) #removing special character and numbers\n",
    "        tokens = remove_punct(words) #removing punctuations and tokanization\n",
    "        tokens = lemmitizar(tokens) #lemmitization\n",
    "        tokens = remove_stopwords(tokens) ##removing stop-words\n",
    "        tokens = lower_case(tokens) #converting to lowercase\n",
    "\n",
    "        #joining words with space and removing single tetter tokens\n",
    "        parag=' '.join( [word for word in tokens if len(word)>1] )\n",
    "        #parag=' '.join( [word for word in tokens if len(word)>1 and word.isalpha()] )#if we dont want numeric values\n",
    "\n",
    "\n",
    "        #inseting data into lists and dictionaries\n",
    "        text.append(parag)\n",
    "        #text_file_path.append(file_path)\n",
    "        labels.append(clss)\n",
    "        data_dict[parag]=clss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11660\n",
      "11660\n",
      "11212\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(labels))\n",
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29 2021 10 42 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 2021 09 34 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29 2021 11 03 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 08 25 alberta health services rrduser 403...</td>\n",
       "      <td>Admin Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahs 2021 23 05 am page alberta health services...</td>\n",
       "      <td>Admin Note</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      target\n",
       "0  29 2021 10 42 alberta health services rrduser ...  Admin Note\n",
       "1  30 2021 09 34 alberta health services rrduser ...  Admin Note\n",
       "2  29 2021 11 03 alberta health services rrduser ...  Admin Note\n",
       "3  2021 08 25 alberta health services rrduser 403...  Admin Note\n",
       "4  ahs 2021 23 05 am page alberta health services...  Admin Note"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame()\n",
    "df[\"text\"]=text\n",
    "df[\"target\"]=labels\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels:   ['Admin Note', 'Clinical History And Summary', 'Consult Note', 'Cover Page', 'Imaging Note', 'Insurance Authorization', 'Intake Forms', 'Lab Test', 'Other', 'Patient Profile', 'Prescriptions', 'Referral Letter', 'Requisition Form']\n",
      "\n",
      "Encoded Labels: [ 0  0  0 ... 12 12 12] \n"
     ]
    }
   ],
   "source": [
    "#Label Encoding\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "encoder.fit(list(df.target))\n",
    "print(f\"Unique Labels:   {list(encoder.classes_)}\")\n",
    "\n",
    "label=encoder.transform(list(df.target))\n",
    "print()\n",
    "print(f\"Encoded Labels: {label} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29 2021 10 42 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 2021 09 34 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29 2021 11 03 alberta health services rrduser ...</td>\n",
       "      <td>Admin Note</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 08 25 alberta health services rrduser 403...</td>\n",
       "      <td>Admin Note</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahs 2021 23 05 am page alberta health services...</td>\n",
       "      <td>Admin Note</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      target  labels\n",
       "0  29 2021 10 42 alberta health services rrduser ...  Admin Note       0\n",
       "1  30 2021 09 34 alberta health services rrduser ...  Admin Note       0\n",
       "2  29 2021 11 03 alberta health services rrduser ...  Admin Note       0\n",
       "3  2021 08 25 alberta health services rrduser 403...  Admin Note       0\n",
       "4  ahs 2021 23 05 am page alberta health services...  Admin Note       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding encoded labels to the dataframe\n",
    "df[\"labels\"]=label\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29 2021 10 42 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 2021 09 34 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29 2021 11 03 alberta health services rrduser ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021 08 25 alberta health services rrduser 403...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahs 2021 23 05 am page alberta health services...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  29 2021 10 42 alberta health services rrduser ...       0\n",
       "1  30 2021 09 34 alberta health services rrduser ...       0\n",
       "2  29 2021 11 03 alberta health services rrduser ...       0\n",
       "3  2021 08 25 alberta health services rrduser 403...       0\n",
       "4  ahs 2021 23 05 am page alberta health services...       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing extra column\n",
    "df = df.drop('target', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels after Encoding:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique Labels after Encoding:  {df.labels.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making separate lists of taxt that has common labels\n",
    "lst_0 = list(df[df['labels']==0]['text'])\n",
    "lst_1 = list(df[df['labels']==1]['text'])\n",
    "lst_2 = list(df[df['labels']==2]['text'])\n",
    "lst_3 = list(df[df['labels']==3]['text'])\n",
    "lst_4 = list(df[df['labels']==4]['text'])\n",
    "lst_5 = list(df[df['labels']==5]['text'])\n",
    "lst_6 = list(df[df['labels']==6]['text'])\n",
    "lst_7 =list(df[df['labels']==7]['text'])\n",
    "lst_8 = list(df[df['labels']==8]['text'])\n",
    "lst_9 = list(df[df['labels']==9]['text'])\n",
    "lst_10 = list(df[df['labels']==10]['text'])\n",
    "lst_11 = list(df[df['labels']==11]['text'])\n",
    "lst_12 = list(df[df['labels']==12]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n",
      "1045\n",
      "1438\n",
      "1542\n",
      "491\n",
      "969\n",
      "23\n",
      "780\n",
      "982\n",
      "72\n",
      "55\n",
      "2363\n",
      "1312\n"
     ]
    }
   ],
   "source": [
    "print(len(lst_0))\n",
    "print(len(lst_1))\n",
    "print(len(lst_2))\n",
    "print(len(lst_3))\n",
    "print(len(lst_4))\n",
    "print(len(lst_5))\n",
    "print(len(lst_6))\n",
    "print(len(lst_7))\n",
    "print(len(lst_8))\n",
    "print(len(lst_9))\n",
    "print(len(lst_10))\n",
    "print(len(lst_11))\n",
    "print(len(lst_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding remaining sample to negitive samples list list so that we can make pairs with positive and negitive samples\n",
    "lst_0_neg= list(itertools.chain(lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_1_neg= list(itertools.chain(lst_0,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_2_neg= list(itertools.chain(lst_0,lst_1,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_3_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_4_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_5_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_6_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_7,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_7_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_8,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_8_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_9,lst_10,lst_11,lst_12))\n",
    "lst_9_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_10,lst_11,lst_12))\n",
    "lst_10_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_11,lst_12))\n",
    "lst_11_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_12))\n",
    "lst_12_neg= list(itertools.chain(lst_0,lst_1,lst_2,lst_3,lst_4,lst_5,lst_6,lst_7,lst_8,lst_9,lst_10,lst_11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns in dataframe: Index(['text1', 'text2', 'label'], dtype='object')\n",
      "\n",
      "length of dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "#making a pandas dataframe\n",
    "columns=['text1', 'text2', 'label']\n",
    "d_frame = pd.DataFrame(columns=columns)\n",
    "print(f'columns in dataframe: {d_frame.columns}')\n",
    "print()\n",
    "print(f\"length of dataframe:  {len(d_frame)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairing\n",
    "def pairing(pos_samples_list,neg_samples_list):\n",
    "\n",
    "    #pairing positive to negitive samples with label 0\n",
    "    for txt_a in pos_samples_list:\n",
    "        for txt_b in neg_samples_list:\n",
    "\n",
    "            data1 = txt_a\n",
    "            data2 = txt_b\n",
    "            d_frame.loc[len(d_frame)] = [data1, data2, 0]\n",
    "    \n",
    "    #pairing positive to positive samples with label 1\n",
    "    for txt_c in pos_samples_list:\n",
    "        for txt_d in pos_samples_list:\n",
    "            data3=txt_c\n",
    "            data4=txt_d\n",
    "            d_frame.loc[len(d_frame)] = [data3, data4, 1]\n",
    "    \n",
    "    return d_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing(lst_0,lst_0_neg)\n",
    "pairing(lst_1,lst_1_neg)\n",
    "pairing(lst_2,lst_2_neg)\n",
    "pairing(lst_3,lst_3_neg)\n",
    "pairing(lst_4,lst_4_neg)\n",
    "pairing(lst_5,lst_5_neg)\n",
    "pairing(lst_6,lst_6_neg)\n",
    "pairing(lst_7,lst_7_neg)\n",
    "pairing(lst_8,lst_8_neg)\n",
    "pairing(lst_9,lst_9_neg)\n",
    "pairing(lst_10,lst_10_neg)\n",
    "pairing(lst_11,lst_11_neg)\n",
    "pairing(lst_12,lst_12_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...</td>\n",
       "      <td>ahs august 2021 39 pm raapid north referral su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...</td>\n",
       "      <td>patient name josie francesca bruce patient id ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...</td>\n",
       "      <td>20210322 15 50 wfcccomm cntr 1111 hfr niagara ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...</td>\n",
       "      <td>20210428 18 54 wfcc comm centre 41023 hfr niag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...</td>\n",
       "      <td>26 04 2021 mon 11 08 lordache irina id 104783 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...   \n",
       "1  o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...   \n",
       "2  o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...   \n",
       "3  o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...   \n",
       "4  o7 29 2021 thu 11 fax joos 016 20200630 15 19 ...   \n",
       "\n",
       "                                               text2 label  \n",
       "0  ahs august 2021 39 pm raapid north referral su...     0  \n",
       "1  patient name josie francesca bruce patient id ...     0  \n",
       "2  20210322 15 50 wfcccomm cntr 1111 hfr niagara ...     0  \n",
       "3  20210428 18 54 wfcc comm centre 41023 hfr niag...     0  \n",
       "4  26 04 2021 mon 11 08 lordache irina id 104783 ...     0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to save dataframe as csv file\n",
    "csv_path='/media/umar_visionx/Backup Plus/Active/Faizan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataframe as csv file\n",
    "d_frame.to_csv(os.path.join(csv_path,'data.csv'),index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataframe\n",
    "data=pd.read_csv(os.path.join(csv_path,'data_medium.csv'))\n",
    "test_data=pd.read_csv(os.path.join(csv_path,'test_short.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training Examples: 187726\n",
      "total validation Examples: 80454\n",
      "total test Examples: 15120\n"
     ]
    }
   ],
   "source": [
    "#splitting in training and validation dataset\n",
    "feat=data[[\"text1\",\"text2\"]]#features\n",
    "lab=data[[\"label\"]]#labels\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(feat, lab, test_size=0.3,random_state=42,stratify=lab)\n",
    "\n",
    "print(f\"total training Examples: {len(x_train)}\")\n",
    "print(f\"total validation Examples: {len(x_val)}\")\n",
    "print(f\"total test Examples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thu fax joos boupe shirlay dos jun dob age cadar ridge cr sw calgary ab geriatric depression seca ns instruction circle answer best describes felt past week are basically satisfied life yes have dropped many activity interest yes fo do feel thet life empty yes do often get bored yes are good spirit time are afraid something bad going happen te yes do feel happy time xe do often ivel helpless do prefer stay home rather going thing yey ono do feel problem memory yes il do think wonderful alive do feel worthless way youare yes do feel full energy do feel situation hopeless yes do think people better yes total score toole may copfed without permiaaion\n",
      "-----------\n",
      "thu fax joos boupe shirlay dos jun dob age cadar ridge cr sw calgary ab geriatric depression seca ns instruction circle answer best describes felt past week are basically satisfied life yes have dropped many activity interest yes fo do feel thet life empty yes do often get bored yes are good spirit time are afraid something bad going happen te yes do feel happy time xe do often ivel helpless do prefer stay home rather going thing yey ono do feel problem memory yes il do think wonderful alive do feel worthless way youare yes do feel full energy do feel situation hopeless yes do think people better yes total score toole may copfed without permiaaion alberta health services rrduser appointment reschedule confirmation foothills medical centre st nw calgary alberta loudfoot darren william phn greenwood cres sw medical record number calgary ab gender male dob jan phone dear dr yuen chu diagnostic imaging appointment ha scheduled patient per request please provide patient performing facility information exam date time preparation detail please aware previous appointment patient ha rescheduled the new appointment detail date time wednesday november pm afternoon location fmc dimri procedure mr cardiac nonenh enhanced rebooked october per pt request pt aware new date time please arrive minute prior appointment time report cardiac mri basement level special services building foothills medical centre based information provided patient ha assigned priority if doe meet expectation mri ct exams call pet ct call please note this phone number physician use mr cardiac imaging patients claustrophobic if oral sedation required provide patient prescription advise patient need driver exam do take sedation medication directed staff patient preparation no preparation required cardiac mri unless stress test instruction indicated you required lie still minute follow simple instruction breathing please avoid coffee caffeine drink hour prior exam special considerations and precautions notify physician ever penetrating eye injury involving metal may need xrays eye appointment program prg page\n"
     ]
    }
   ],
   "source": [
    "#merging text1 and text2\n",
    "x_train['text'] = x_train[['text1', 'text2']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)\n",
    "print(x_train['text1'][0])\n",
    "print(\"-----------\")\n",
    "print(x_train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187726\n",
      "187726\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train['text1']))\n",
    "print(len(x_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(x_train['text'])\n",
    "words_to_index = tokenizer.word_index#getting the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115935\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(words_to_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting type to avoid(AttributeError: 'float' object has no attribute 'lower') in next cell\n",
    "x_train['text1']=x_train['text1'].astype(str)\n",
    "x_train['text2']=x_train['text2'].astype(str)\n",
    "x_val['text1']=x_val['text1'].astype(str)\n",
    "x_val['text2']=x_val['text2'].astype(str)\n",
    "test_data['text1']=test_data['text1'].astype(str)\n",
    "test_data['text2']=test_data['text2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text1_seq = tokenizer.texts_to_sequences(x_train[\"text1\"])#we can use x_train['text1'].values to use only text this will convert x_train['text1'] in to array containing all texts\n",
    "tr_text2_seq = tokenizer.texts_to_sequences(x_train[\"text2\"])\n",
    "val_text1_seq = tokenizer.texts_to_sequences(x_val[\"text1\"])\n",
    "val_text2_seq = tokenizer.texts_to_sequences(x_val[\"text2\"])\n",
    "test_text1_seq=tokenizer.texts_to_sequences(test_data[\"text1\"])\n",
    "test_text2_seq=tokenizer.texts_to_sequences(test_data[\"text2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n",
      "765\n",
      "273\n",
      "765\n",
      "327\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "#Getting maximum sequence length for padding\n",
    "seq_l1 = max([len(x) for x in tr_text1_seq])\n",
    "seq_l2 = max([len(x) for x in tr_text2_seq])\n",
    "seq_l3 = max([len(x) for x in val_text1_seq])\n",
    "seq_l4 = max([len(x) for x in val_text2_seq])\n",
    "seq_l5=max([len(x) for x in test_text1_seq])\n",
    "seq_l6=max([len(x) for x in test_text2_seq])\n",
    "print(seq_l1)\n",
    "print(seq_l2)\n",
    "print(seq_l3)\n",
    "print(seq_l4)\n",
    "print(seq_l5)\n",
    "print(seq_l6)\n",
    "#we will use seq_l2 because it is maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding for training\n",
    "#padding upto maximum sequence length\n",
    "#max len is 381\n",
    "tr_text1_seq_pad = pad_sequences(tr_text1_seq, padding='post', truncating='post', maxlen=seq_l2)\n",
    "tr_text2_seq_pad = pad_sequences(tr_text2_seq, padding='post', truncating='post', maxlen=seq_l2)\n",
    "val_text1_seq_pad = pad_sequences(val_text1_seq, padding='post', truncating='post', maxlen=seq_l2)\n",
    "val_text2_seq_pad = pad_sequences(val_text2_seq, padding='post', truncating='post', maxlen=seq_l2)\n",
    "test_text1_seq_pad=pad_sequences(test_text1_seq, padding='post', truncating='post', maxlen=seq_l2)\n",
    "test_text2_seq_pad=pad_sequences(test_text2_seq, padding='post', truncating='post', maxlen=seq_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "sequences after padding:  [[143 915   1 ...   0   0   0]\n",
      " [143   1  10 ...   0   0   0]\n",
      " [143   1  10 ...   0   0   0]\n",
      " ...\n",
      " [409  10  24 ...   0   0   0]\n",
      " [143 915   1 ...   0   0   0]\n",
      " [409  10  24 ...   0   0   0]]\n",
      "--------------\n",
      "padded seq shape:  (187726, 765)\n",
      "--------------\n",
      "data type of sequence:  <class 'list'>\n",
      "--------------\n",
      "data type of padded seq: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#checking the results\n",
    "#print(\"word's index: \", words_to_index)\n",
    "#print(\"-------------\")\n",
    "#print(\"sequences:\", tr_text1_seq)\n",
    "print(\"--------------\")\n",
    "print(\"sequences after padding: \", tr_text1_seq_pad)\n",
    "print(\"--------------\")\n",
    "print(\"padded seq shape: \", tr_text1_seq_pad.shape)\n",
    "print(\"--------------\")\n",
    "print(\"data type of sequence: \", type(tr_text1_seq))\n",
    "print(\"--------------\")\n",
    "print(\"data type of padded seq:\", type(tr_text1_seq_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained word embeddings(Glove Embedding)\n",
    "#For more info check https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "#For more info check https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Next, we compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:\n",
    "#For more info check https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "main_path='/media/umar_visionx/Backup Plus/Active/Faizan'\n",
    "path_to_glove_file = os.path.join(main_path,\"glove.6B.100d.txt\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For more info check https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "EMBEDDING_DIM=len(embeddings_index['patient'])#getting embedding dimensions with any word\n",
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of embedding metrix: <class 'numpy.ndarray'>\n",
      "----------\n",
      "(115935, 100)\n",
      "shape of embedding metrix: None\n",
      "----------\n",
      "embedding metrix: [[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.18970001  0.050024    0.19084001 ... -0.39804     0.47646999\n",
      "  -0.15983   ]\n",
      " [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n",
      "   0.27061999]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.65524     0.42197001  0.050696   ... -0.032223   -0.55261999\n",
      "  -0.12278   ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:\n",
    "#For more info check https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in words_to_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(f\"type of embedding metrix: {type(embedding_matrix)}\")\n",
    "print(\"----------\")\n",
    "print(f\"shape of embedding metrix: {print(embedding_matrix.shape)}\")\n",
    "print(\"----------\")\n",
    "print(f\"embedding metrix: {embedding_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load this embedding matrix into an Embedding layer. Note that we set trainable=False to prevent the weights from being updated during training.\n",
    "#For more info check https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html and https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "def siamese_model(input_shape):\n",
    "\n",
    "    first_inp = Input(input_shape)\n",
    "    second_inp = Input(input_shape)\n",
    "\n",
    "    model=Sequential()\n",
    "    #check the documentaation of keras Embedding layer\n",
    "    #input_dim=vocab_size=4534,output_dim=EMBEDDING_DIM=100,input_length=seq_l2=tr_text1_seq_pad.shape(1)\n",
    "    model.add(Embedding(vocab_size,EMBEDDING_DIM,weights=[embedding_matrix],input_length=seq_l2,trainable=False))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    # Generate the encodings (feature vectors) for two sequences\n",
    "    encoded_first = model(first_inp)\n",
    "    encoded_second = model(second_inp)\n",
    "\n",
    "    #cosine simmilarity\n",
    "    # cosine_similarity = 1 - cosine(encoded_first, encoded_second)\n",
    "\n",
    "    \n",
    "    L1_layer = Lambda(lambda tensors:abs(tensors[0] - tensors[1]))\n",
    "    dist = L1_layer([encoded_first, encoded_second]) #L1_distance\n",
    "\n",
    "     # generating the similarity score\n",
    "    outputs = Dense(1,activation='tanh')(dist)#prediction\n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_mod = Model(inputs=[first_inp,second_inp],outputs=outputs)\n",
    "    return siamese_mod\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_text1_seq_pad.shape[1]#input shape will be(None,381)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model = siamese_model(tr_text1_seq_pad.shape[1])#as both seq have ssame sahpes so we are only using one argument\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=MeanSquaredError(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(187726, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(187726, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187726, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train.reshape(-1,1)\n",
    "y_val.reshape(-1,1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early-Stopping/Callback\n",
    "callback = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 187726 samples, validate on 80454 samples\n",
      "Epoch 1/10\n",
      "187726/187726 [==============================] - 100s 533us/sample - loss: 0.0021 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 2/10\n",
      "187726/187726 [==============================] - 99s 530us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 3/10\n",
      "187726/187726 [==============================] - 100s 532us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 4/10\n",
      "187726/187726 [==============================] - 100s 532us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 5/10\n",
      "187726/187726 [==============================] - 100s 532us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 6/10\n",
      "187726/187726 [==============================] - 102s 545us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 7/10\n",
      "187726/187726 [==============================] - 102s 541us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 8/10\n",
      "187726/187726 [==============================] - 101s 537us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 9/10\n",
      "187726/187726 [==============================] - 101s 536us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n",
      "Epoch 10/10\n",
      "187726/187726 [==============================] - 99s 529us/sample - loss: 0.0020 - acc: 0.9980 - val_loss: 0.0020 - val_acc: 0.9980\n"
     ]
    }
   ],
   "source": [
    "# model traning\n",
    "history=model.fit([tr_text1_seq_pad,tr_text2_seq_pad], y_train, batch_size=64, epochs=10, validation_data=([val_text1_seq_pad, val_text2_seq_pad],y_val), verbose=1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcd542f5cd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZQU9b3+8fcjjCyCIEsUQQUjigsCYQQUjSa4gKL4U6K4470Jx6gRjRrRGGOM5pobk1y9QYwa44YLgaAkIZqriN64MhhQQRTkojOuiIIQHVn8/P7ogjRDDww4NT10Pa9z5pzuqm9Vfaoa+un6Vnd9FRGYmVl2bVPsAszMrLgcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAmu0JN0p6do6tl0k6fC0ayo1kg6TVFXsOqy4HARmJURSX0kzJa2Q9Lqko4pdkzV+TYtdgFmpk9Q0IlY30OZ+A/wVOADYDWjZQNu1rZjPCOxLSbpkLpX0kqR/SvqdpB0l/VXSckmPSdohr/1xkuZIWippuqS98+b1kfRistyDQPMa2xoqaVay7DOS9q9jjcdI+oekTyRVSrq6xvyDk/UtTeaPTKa3kPRLSW9KWibp78m0DbpT8rumJF0taaKkeyV9AoyU1E/Ss8k23pX0G0nb5i2/r6T/kfSRpPclXSFpJ0mfSmqf166vpMWSymrZ3dXAmxHxRUT8X0TMqcsxylv/3snrsjR5nY7Lm3e0pLnJ6/O2pEuS6R0k/TlZ5iNJ/yvJ7y1bk4jwn/+2+A9YBDwH7Ah0Bj4AXgT6AM2AacCPk7Z7Av8EjgDKgB8AC4Btk783gYuSecOBVcC1ybJfS9bdH2gCnJVsu1leHYfXUuNhQE9yH3z2B94Hjk/m7QosB05Jttse6J3MGwtMT/arCXBQsk+HAVUFjsPhyeOrk9qPT7bZAugLDCB3Ft4VeBW4MGnfGngXuJhc+LUG+ifzpgLfzdvOr4H/3sjr8UvgY6BPHV+/dfuS7P8C4Irk9fhmcmz2Sua/CxySPN4B+Fry+D+AW5Lly4BDABX736b/6v7n1Lb68N8R8X5EvA38L/B8RPwjIj4HJpMLBYCTgb9ExP9ExCrgBnJvkgeRe5MsA/4rIlZFxERgRt42vgP8NiKej4g1EXEX8Hmy3EZFxPSIeDlyn5JfAu4HDk1mnwY8FhH3J9tdEhGzkk+0/waMjoi3k20+k+xTXTwbEQ8l2/wsImZGxHMRsToiFgG/zathKPBeRPwyIqojYnlEPJ/Muws4HUBSE3KBdU+hDUoaAXwjafMnSX2S6UdImlmHmgcArYDrI2JlREwD/pysD3Lhto+k7SPi44h4MW96J2C35Bj+b0T4JmZbEQeB1Yf38x5/VuB5q+TxzuQ+9QMQEV8AleQ+ce8MvF3jDeTNvMe7ARcn3Q9LJS0FdkmW2yhJ/SU9kXSpLAPOAToks3cB3iiwWAdyn84LzauLyho17Jl0n7yXdBf9rA41ADxM7s13d3JnUssi4oVa2o4GfhMRj5Dbx0eSMDgIeKwONe8MVCavy1pvknt9AE4EjgbelPSkpAOT6b8gdybxN0kLJY2pw7asEXEQWEN6h9wbOgCSRO5N8G1y3Q6dk2lr7Zr3uBK4LiLa5v21jIj767Dd+4ApwC4R0YZcN8ba7VQCXy2wzIdAdS3z/kneRdjkk3rHGm1qfiIeB8wDukfE9uS6XzZVAxFRDUwgd+ZyBrWcDSSakrtGQET8Gfg+8DdgJPCrjSy31jvALjX693cl9/oQETMiYhjwFeChpC6SM5iLI2J34Fjg+5IG1WF71kg4CKwhTQCOkTQoudh5MbnunWeAZ8m9iV0gqamkE4B+ecveBpyTfLqXpO2Si8Ct67Dd1sBHEVEtqR9wat688cDhkk5KttteUu/kU/EdwK8k7SypiaQDJTUDXgeaJ9svA64kd+1gUzV8AqyQ1AP4bt68PwM7SbpQUjNJrSX1z5t/N7k38+OAezeyjT8AV0nqlbyZv07ujGw7alx4r8Xz5ELuB5LKJB1G7o39AUnbSjpNUpukW+8TYA2su4i/RxLia6evqcP2rJFwEFiDiYjXyPV3/ze5T9zHAscm/dErgRPIveF9TO56wh/zlq0gd53gN8n8BUnbujgXuEbScuAqkk+yyXrfItfdcTHwETAL6JXMvgR4mdy1io+AnwPbRMSyZJ23k/u0/E9gUz/KuoRcAC0nF2oP5tWwnFy3z7HAe8B8cn39a+c/DXwBvJhcX6jNDeTCa3JS703kuojuAv4iqc3GCkxeg+OAIeRen5uBMyNiXtLkDGBR0rV1Dsm1C6A7ua6nFeQC/eaImL6xbVnjIl/TMWv8JE0D7ouI24tdi5UeB4FZIyfpAOB/yF3jWF7seqz0uGvIrBGTdBe5bpcLHQKWFp8RmJllnM8IzMwybqu76VyHDh2ia9euxS7DzGyrMnPmzA8joubvXYCtMAi6du1KRUVFscswM9uqSHqztnnuGjIzyzgHgZlZxjkIzMwybqu7RlDIqlWrqKqqorq6utilbPWaN29Oly5dKCurbdwTMys1JREEVVVVtG7dmq5du7L+zSttc0QES5Ysoaqqim7duhW7HDNrIKl1DUm6Q9IHkl6pZb4k3SRpgXLDHH5tS7dVXV1N+/btHQJfkiTat2/vMyuzjEnzGsGdwOCNzB9C7q6F3YFR5O7XvsUcAvXDx9Ese1LrGoqIpyR13UiTYcDdyYhUz0lqK6lTRLybRj1L31qAPl+VxqpLzmcfvMujl5xV7DLMrIZVX+3C0Jv+uOmGm6mY3xrqzPrD+VXxryHx1iNplKQKSRWLFy9ukOLMzLKimBeLC/VBFLwDXkTcCtwKUF5evkV3yWu76x5bslidLF26lPvuu49zzz13s5Y7+uijue+++2jbtu1mLTdy5EiGDh3K8OHDN2u5umqxGo76S23D4ppZqSnmGUEVufFq1+pCbszUrc7SpUu5+eabN5i+Zs3GR+ubOnXqZoeAmVl9K+YZwRTgfEkPAP2BZfVxfeAnf5rD3Hc++dLF5dtn5+358bH71jp/zJgxvPHGG/Tu3ZuysjJatWpFp06dmDVrFnPnzuX444+nsrKS6upqRo8ezahRo4B/3TdpxYoVDBkyhIMPPphnnnmGzp078/DDD9OiRYtN1vb4449zySWXsHr1ag444ADGjRtHs2bNGDNmDFOmTKFp06YceeSR3HDDDfzhD3/gJz/5CU2aNKFNmzY89dRT9XaMzGzrlVoQSLofOAzoIKkK+DFQBhARtwBTyY0VuwD4FDg7rVrSdv311/PKK68wa9Yspk+fzjHHHMMrr7yy7rv4d9xxB+3ateOzzz7jgAMO4MQTT6R9+/brrWP+/Pncf//93HbbbZx00klMmjSJ008/vdDm1qmurmbkyJE8/vjj7Lnnnpx55pmMGzeOM888k8mTJzNv3jwksXTpUgCuueYaHn30UTp37rxumplZmt8aOmUT8wM4r763u7FP7g2lX79+6/0g66abbmLy5MkAVFZWMn/+/A2CoFu3bvTu3RuAvn37smjRok1u57XXXqNbt27sueeeAJx11lmMHTuW888/n+bNm/Ptb3+bY445hqFDhwIwcOBARo4cyUknncQJJ5xQH7tqZiXA9xpKwXbbbbfu8fTp03nsscd49tlnmT17Nn369Cn4g61mzZqte9ykSRNWr169ye3UNrpc06ZNeeGFFzjxxBN56KGHGDw493OOW265hWuvvZbKykp69+7NkiVLNnfXzKwElcQtJoqtdevWLF9eeDjZZcuWscMOO9CyZUvmzZvHc889V2/b7dGjB4sWLWLBggXsscce3HPPPRx66KGsWLGCTz/9lKOPPpoBAwawxx65b0y98cYb9O/fn/79+/OnP/2JysrKDc5MzCx7HAT1oH379gwcOJD99tuPFi1asOOOO66bN3jwYG655Rb2339/9tprLwYMGFBv223evDm///3v+da3vrXuYvE555zDRx99xLBhw6iuriYi+PWvfw3ApZdeyvz584kIBg0aRK9eveqtFjPbem11g9eXl5dHzRHKXn31Vfbee+8iVVR6fDzNSo+kmRFRXmierxGYmWWcu4YasfPOO4+nn356vWmjR4/m7LO32m/amlkj5CBoxMaOHVvsEswsA9w1ZGaWcQ4CM7OMcxCYmWWcg8DMLOMcBEXQqlWrWuctWrSI/fbbrwGrMbOscxCYmWVc6X199K9j4L2X63edO/WEIdfXOvuyyy5jt912WzdC2dVXX40knnrqKT7++GNWrVrFtddey7BhwzZrs9XV1Xz3u9+loqKCpk2b8qtf/YpvfOMbzJkzh7PPPpuVK1fyxRdfMGnSJHbeeWdOOukkqqqqWLNmDT/60Y84+eSTv9Rum1k2lF4QFMGIESO48MIL1wXBhAkTeOSRR7jooovYfvvt+fDDDxkwYADHHXccUqEROgtb+zuCl19+mXnz5nHkkUfy+uuvc8sttzB69GhOO+00Vq5cyZo1a5g6dSo777wzf/nLX4Dcze7MzOqi9IJgI5/c09KnTx8++OAD3nnnHRYvXswOO+xAp06duOiii3jqqafYZpttePvtt3n//ffZaaed6rzev//973zve98Dcnca3W233Xj99dc58MADue6666iqquKEE06ge/fu9OzZk0suuYTLLruMoUOHcsghh6S1u2ZWYnyNoJ4MHz6ciRMn8uCDDzJixAjGjx/P4sWLmTlzJrNmzWLHHXcsOA7BxtR2Q8BTTz2VKVOm0KJFC4466iimTZvGnnvuycyZM+nZsyeXX34511xzTX3slpllQOmdERTJiBEj+M53vsOHH37Ik08+yYQJE/jKV75CWVkZTzzxBG+++eZmr/PrX/8648eP55vf/Cavv/46b731FnvttRcLFy5k991354ILLmDhwoW89NJL9OjRg3bt2nH66afTqlUr7rzzzvrfSTMrSQ6CerLvvvuyfPlyOnfuTKdOnTjttNM49thjKS8vp3fv3vTo0WOz13nuuedyzjnn0LNnT5o2bcqdd95Js2bNePDBB7n33nspKytjp5124qqrrmLGjBlceumlbLPNNpSVlTFu3LgU9tLMSpHHI7AN+HialR6PR2BmZrVy11CRvPzyy5xxxhnrTWvWrBnPP/98kSoys6xyEBRJz549mTVrVrHLMDNz15CZWdY5CMzMMs5BYGaWcQ4CM7OMcxAUwcbGIzAza2gOAjOzjCu5r4/+/IWfM++jefW6zh7tenBZv8tqnV+f4xGsWLGCYcOGFVzu7rvv5oYbbkAS+++/P/fccw/vv/8+55xzDgsXLgRg3LhxHHTQQfWw12aWFakGgaTBwI1AE+D2iLi+xvxdgbuAtkmbMRExNc2a0lCf4xE0b96cyZMnb7Dc3Llzue6663j66afp0KEDH330EQAXXHABhx56KJMnT2bNmjWsWLEi9f01s9KSWhBIagKMBY4AqoAZkqZExNy8ZlcCEyJinKR9gKlA1y+z3Y19ck9LfY5HEBFcccUVGyw3bdo0hg8fTocOHQBo164dANOmTePuu+8GoEmTJrRp0ybdnTWzkpPmGUE/YEFELASQ9AAwDMgPggC2Tx63Ad5JsZ5UrR2P4L333ttgPIKysjK6du1ap/EIalsuIjZrdDMzs7pK82JxZ6Ay73lVMi3f1cDpkqrInQ18L8V6UjVixAgeeOABJk6cyPDhw1m2bNkWjUdQ23KDBg1iwoQJLFmyBGBd19CgQYPW3XJ6zZo1fPLJJynsnZmVsjSDoNDH15r3vD4FuDMiugBHA/dI2qAmSaMkVUiqWLx4cQqlfnmFxiOoqKigvLyc8ePH13k8gtqW23ffffnhD3/IoYceSq9evfj+978PwI033sgTTzxBz5496du3L3PmzEltH82sNKU2HoGkA4GrI+Ko5PnlABHxH3lt5gCDI6Iyeb4QGBARH9S2Xo9HkD4fT7PSU6zxCGYA3SV1k7QtMAKYUqPNW8CgpMi9geZA4/zIb2ZWolK7WBwRqyWdDzxK7quhd0TEHEnXABURMQW4GLhN0kXkuo1GxtY2ZNoW8ngEZtZYpPo7guQ3AVNrTLsq7/FcYGCaNTRWHo/AzBoL32LCzCzjHARmZhnnIDAzyzgHgZlZxjkI6sHSpUu5+eabN3u5o48+mqVLl6ZQkZlZ3TkI6kFtQbBmzZqNLjd16lTatm2bVllmZnVScuMRvPezn/H5q/U7HkGzvXuw0xVX1Dp/zJgxvPHGG/Tu3ZuysjJatWpFp06dmDVrFnPnzuX444+nsrKS6upqRo8ezahRowDo2rUrFRUVrFixgiFDhnDwwQfzzDPP0LlzZx5++GFatGhRcHu33XYbt956KytXrmSPPfbgnnvuoWXLlrWOTVBoHAMzs7V8RlAPrr/+er761a8ya9YsfvGLX/DCCy9w3XXXMXdu7kard9xxBzNnzqSiooKbbrpp3Y3j8s2fP5/zzjuPOXPm0LZtWyZNmlTr9k444QRmzJjB7Nmz2Xvvvfnd734H/GtsgtmzZ/Piiy+y7777MmfOHK677jqmTZvG7NmzufHGG9M5CGa21Sq5M4KNfXJvKP369aNbt27rnt90001MnjwZgMrKSubPn0/79u3XW6Zbt2707t0bgL59+7Jo0aJa1//KK69w5ZVXsnTpUlasWMFRRx0FFB6b4O677y44joGZ2VolFwSNwXbbbbfu8fTp03nsscd49tlnadmyJYcddljBcQmaNWu27nGTJk347LPPal3/yJEjeeihh+jVqxd33nkn06dPr7WtxzEws01x11A9aN26NcuXLy84b9myZeywww60bNmSefPm8dxzz33p7S1fvpxOnTqxatUqxo8fv256obEJahvHwMxsLQdBPWjfvj0DBw5kv/3249JLL11v3uDBg1m9ejX7778/P/rRjxgwYMCX3t5Pf/pT+vfvzxFHHLHeOAeFxiaobRwDM7O1UhuPIC0ejyB9Pp5mpadY4xGYmdlWwBeLG7HzzjuPp59+er1po0eP5uyzzy5SRWZWikomCErx2zFjx45t8G1ubV2FZvbllUTXUPPmzVmyZInfxL6kiGDJkiU0b9682KWYWQMqiTOCLl26UFVVxeLFHu74y2revDldunQpdhlm1oBKIgjKysrW+yWvmZnVXUl0DZmZ2ZZzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGZdqEEgaLOk1SQskjamlzUmS5kqaI+m+NOsxM7MNpXb3UUlNgLHAEUAVMEPSlIiYm9emO3A5MDAiPpb0lbTqMTOzwtI8I+gHLIiIhRGxEngAGFajzXeAsRHxMUBEfJBiPWZmVkCaQdAZqMx7XpVMy7cnsKekpyU9J2lwoRVJGiWpQlKFB58xM6tfaQZBoQGEa44l2RToDhwGnALcLqntBgtF3BoR5RFR3rFjx3ov1Mwsy9IMgipgl7znXYB3CrR5OCJWRcT/Aa+RCwYzM2sgaQbBDKC7pG6StgVGAFNqtHkI+AaApA7kuooWpliTmZnVkFoQRMRq4HzgUeBVYEJEzJF0jaTjkmaPAkskzQWeAC6NiCVp1WRmZhtSRM1u+8atvLw8Kioqil2GmdlWRdLMiCgvNM+/LDYzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZV6cgkDRa0vbK+Z2kFyUdmXZxZmaWvrqeEfxbRHwCHAl0BM4Grk+tKjMzazB1DYK1t5Q+Gvh9RMym8G2mzcxsK1PXIJgp6W/kguBRSa2BL9Iry8zMGkpdxyz+d6A3sDAiPpXUjlz3kJmZbeXqekZwIPBaRCyVdDpwJbAsvbLMzKyh1DUIxgGfSuoF/AB4E7g7tarMzKzB1DUIVkdu4IJhwI0RcSPQOr2yzMysodT1GsFySZcDZwCHSGoClKVXlpmZNZS6nhGcDHxO7vcE7wGdgV+kVpWZmTWYOgVB8uY/HmgjaShQHRG+RmBmVgLqeouJk4AXgG8BJwHPSxqeZmFmZtYw6nqN4IfAARHxAYCkjsBjwMS0CjMzs4ZR12sE26wNgcSSzVjWzMwasbqeETwi6VHg/uT5ycDUdEoyM7OGVKcgiIhLJZ0IDCR3s7lbI2JyqpWZmVmDqOsZARExCZiUYi1mZlYEGw0CScuBKDQLiIjYPpWqzMyswWw0CCLCt5EwMytx/uaPmVnGOQjMzDLOQWBmlnEOAjOzjEs1CCQNlvSapAWSxmyk3XBJIak8zXrMzGxDqQVBMmbBWGAIsA9wiqR9CrRrDVwAPJ9WLWZmVrs0zwj6AQsiYmFErAQeIDfCWU0/Bf4TqE6xFjMzq0WaQdAZqMx7XpVMW0dSH2CXiPjzxlYkaZSkCkkVixcvrv9KzcwyLM0gUIFp636lLGkb4NfAxZtaUUTcGhHlEVHesWPHeizRzMzSDIIqYJe8512Ad/Ketwb2A6ZLWgQMAKb4grGZWcNKMwhmAN0ldZO0LTACmLJ2ZkQsi4gOEdE1IroCzwHHRURFijWZmVkNqQVBRKwGzgceBV4FJkTEHEnXSDoure2amdnmqfNtqLdEREylxgA2EXFVLW0PS7MWMzMrzL8sNjPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllXKpBIGmwpNckLZA0psD870uaK+klSY9L2i3NeszMbEOpBYGkJsBYYAiwD3CKpH1qNPsHUB4R+wMTgf9Mqx4zMysszTOCfsCCiFgYESuBB4Bh+Q0i4omI+DR5+hzQJcV6zMysgDSDoDNQmfe8KplWm38H/ppiPWZmVkDTFNetAtOiYEPpdKAcOLSW+aOAUQC77rprfdVnZmake0ZQBeyS97wL8E7NRpIOB34IHBcRnxdaUUTcGhHlEVHesWPHVIo1M8uqNINgBtBdUjdJ2wIjgCn5DST1AX5LLgQ+SLEWMzOrRWpBEBGrgfOBR4FXgQkRMUfSNZKOS5r9AmgF/EHSLElTalmdmZmlJM1rBETEVGBqjWlX5T0+PM3tm5nZpvmXxWZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllXKpBIGmwpNckLZA0psD8ZpIeTOY/L6lrmvWYmdmGUgsCSU2AscAQYB/gFEn71Gj278DHEbEH8Gvg52nVY2ZmhTVNcd39gAURsRBA0gPAMGBuXpthwNXJ44nAbyQpIqK+i3l+0o3s+Mqt9b1aqw8qdgHWmDWGfx7RKKqAD/teSPkx36739aYZBJ2ByrznVUD/2tpExGpJy4D2wIf5jSSNAkYB7LrrrltUTPM2Hfm41R5btGx9q/eU25rVf+ablaxmrdulst40g6BQhNb8X1+XNkTErcCtAOXl5Vv0ztHr8FPh8FO3ZFEzs5KW5sXiKmCXvOddgHdqayOpKdAG+CjFmszMrIY0g2AG0F1SN0nbAiOAKTXaTAHOSh4PB6alcX3AzMxql1rXUNLnfz7wKNAEuCMi5ki6BqiIiCnA74B7JC0gdyYwIq16zMyssDSvERARU4GpNaZdlfe4GvhWmjWYmdnG+ZfFZmYZ5yAwM8s4B4GZWcY5CMzMMk5b27c1JS0G3tzCxTtQ41fLGefjsT4fj3/xsVhfKRyP3SKiY6EZW10QfBmSKiKivNh1NBY+Huvz8fgXH4v1lfrxcNeQmVnGOQjMzDIua0Hg+1Cvz8djfT4e/+Jjsb6SPh6ZukZgZmYbytoZgZmZ1eAgMDPLuMwEgaTBkl6TtEDSmGLXUyySdpH0hKRXJc2RNLrYNTUGkppI+oekPxe7lmKT1FbSREnzkn8nBxa7pmKRdFHy/+QVSfdLal7smtKQiSCQ1AQYCwwB9gFOkbRPcasqmtXAxRGxNzAAOC/DxyLfaODVYhfRSNwIPBIRPYBeZPS4SOoMXACUR8R+5G6nX5K3ys9EEAD9gAURsTAiVgIPAMOKXFNRRMS7EfFi8ng5uf/knYtbVXFJ6gIcA9xe7FqKTdL2wNfJjRVCRKyMiKXFraqomgItkhEUW7LhKIslIStB0BmozHteRcbf/AAkdQX6AM8Xt5Ki+y/gB8AXxS6kEdgdWAz8Pukqu13SdsUuqhgi4m3gBuAt4F1gWUT8rbhVpSMrQaAC0zL9vVlJrYBJwIUR8Umx6ykWSUOBDyJiZrFraSSaAl8DxkVEH+CfQCavqUnagVzPQTdgZ2A7SacXt6p0ZCUIqoBd8p53oURP8epCUhm5EBgfEX8sdj1FNhA4TtIicl2G35R0b3FLKqoqoCoi1p4lTiQXDFl0OPB/EbE4IlYBfwQOKnJNqchKEMwAukvqJmlbchd8phS5pqKQJHL9v69GxK+KXU+xRcTlEdElIrqS+3cxLSJK8lNfXUTEe0ClpL2SSYOAuUUsqZjeAgZIapn8vxlEiV44T3XM4sYiIlZLOh94lNyV/zsiYk6RyyqWgcAZwMuSZiXTrkjGlzYD+B4wPvnQtBA4u8j1FEVEPC9pIvAiuW/b/YMSvdWEbzFhZpZxWekaMjOzWjgIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwCxlkg7zXU2tMXMQmJllnIPALCHpdEkvSJol6bfJGAUrJP1S0ouSHpfUMWnbW9Jzkl6SNDm5Lw2S9pD0mKTZyTJfTVbfKu8e/+OTX6oi6XpJc5P13FCkXbeMcxCYAZL2Bk4GBkZEb2ANcBqwHfBiRHwNeBL4cbLI3cBlEbE/8HLe9DV4MVIAAAFkSURBVPHA2IjoRe6+NO8m0/sAF5IbD2N3YKCkdsD/A/ZN1nNtuntpVpiDwCxnENAXmJHcemMQuTfsL4AHkzb3AgdLagO0jYgnk+l3AV+X1BroHBGTASKiOiI+Tdq8EBFVEfEFMAvoCnwCVAO3SzoBWNvWrEE5CMxyBNwVEb2Tv70i4uoC7TZ2T5ZCtztf6/O8x2uAphGxmtygSZOA44FHNrNms3rhIDDLeRwYLukrAJLaSdqN3P+R4UmbU4G/R8Qy4GNJhyTTzwCeTMZ1qJJ0fLKOZpJa1rbBZEyINskN/y4EeqexY2abkom7j5ptSkTMlXQl8DdJ2wCrgPPIDcyyr6SZwDJy1xEAzgJuSd7o8+/QeQbwW0nXJOv41kY22xp4OBkQXcBF9bxbZnXiu4+abYSkFRHRqth1mKXJXUNmZhnnMwIzs4zzGYGZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWXc/wcUNSAoSLQyyAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plotting history\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.title('model accuracy & loss')\n",
    "plt.legend(['train_loss', 'val_loss','val_acc','train_acc'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.curdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "# model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model\n",
    "# model = load_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15120,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15120, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data['label'].shape)\n",
    "test_label=test_data['label'].to_numpy().reshape(-1,1)\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "15120/15120 [==============================] - 3s 173us/sample - loss: 0.0054 - acc: 0.9946\n",
      "test loss, test acc: [0.005418664555269042, 0.99464285]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate([test_text1_seq_pad,test_text2_seq_pad], test_label, batch_size=64)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# # Generate predictions (probabilities -- the output of the last layer)\n",
    "# # on new data using `predict`\n",
    "# print(\"Generate predictions for 3 samples\")\n",
    "# predictions = model.predict([test_text1_seq_pad[:5],test_text2_seq_pad[:5]])\n",
    "# print(\"predictions shape:\", predictions.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
